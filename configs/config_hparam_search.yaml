# Hyperparameter Search Configuration
# Optimized settings for transfer-aware Stage 1 optimization

# Reproducibility seed
seed: 42

# HPO minimal storage mode (set to true by search scripts to skip heavy saves)
is_hparam_search: false

# ============================================================================
# Paths
# ============================================================================
paths:
  # Input data (existing Data/ directory)
  dft_chi_csv: "Data/OMG_DFT_COSMOC_chi.csv"
  exp_chi_csv: "Data/Experiment_chi_data.csv"
  solubility_csv: "Data/Binary_solubility.csv"

  # Processed data cache
  processed_dir: "data/processed"

  # Results output (will be set by search script)
  results_dir: "results/hparam_search"

# ============================================================================
# Chemistry: Featurization parameters
# ============================================================================
chem:
  morgan_radius: 2
  morgan_n_bits: 2048
  descriptor_list:
    - "MolWt"
    - "MolLogP"
    - "TPSA"
    - "NumHDonors"
    - "NumHAcceptors"
    - "FractionCSP3"
    - "NumAromaticRings"
    - "NumAliphaticRings"
    - "NumRotatableBonds"
    - "NumHeteroatoms"
    - "FormalCharge"
    - "HeavyAtomCount"
    - "RingCount"
  smiles_dummy_replacement: "C"
  force_recompute_features: false

# ============================================================================
# Training: Settings optimized for hyperparameter search
# ============================================================================
training:
  device: "cuda"

  # Batch sizes
  batch_size_dft: 256
  batch_size_exp: 64
  batch_size_sol: 64

  # Number of epochs
  num_epochs_pretrain: 100      # Reduced from 200 for faster search
  num_epochs_finetune: 200      # Full training for Stage 2 (not used in quick search)

  # Quick training settings (for Stage 2 CV)
  num_epochs_quick: 20          # Quick Stage 2 evaluation
  quick_patience: 10            # Early stopping for quick training

  # Optimizer
  optimizer: "adamw"

  # Learning rates (will be sampled by Optuna for Stage 1)
  lr_pretrain: 1.0e-3          # Default, will be overridden
  lr_finetune: 3.0e-4          # Fixed for Stage 2

  # Transfer learning: Fixed for Stage 2
  use_discriminative_lr: true
  lr_encoder: 1.0e-5
  freeze_encoder_stage2: true

  # Regularization (will be sampled by Optuna for Stage 1)
  weight_decay: 1.0e-4         # Default, will be overridden

  # Learning rate scheduler (aggressive for faster convergence)
  use_scheduler: true
  scheduler_type: "reduce_on_plateau"
  scheduler_patience: 5        # Reduced from 10 for faster search
  scheduler_factor: 0.5
  scheduler_step_size: 30

  # Early stopping (aggressive for faster search)
  early_stopping: true
  early_stopping_patience: 10  # Reduced from 20

  # DataLoader settings
  num_workers: 4
  pin_memory: true

  # Gradient clipping
  grad_clip_norm: 1.0

  # Checkpoint saving (minimal for search)
  save_best_only: true
  save_interval: 10

# ============================================================================
# Model: Architecture hyperparameters (will be sampled by Optuna)
# ============================================================================
model:
  # Shared encoder (defaults, will be overridden)
  encoder_hidden_dims: [512, 256]
  encoder_latent_dim: 128
  encoder_dropout: 0.2
  encoder_activation: "relu"
  encoder_use_batchnorm: true

  # χ(T) head (defaults, will be overridden)
  chi_head_hidden_dim: 64
  chi_head_dropout: 0.1
  chi_head_activation: "relu"

  # Solubility head (fixed for Stage 2)
  sol_head_hidden_dim: 64
  sol_head_dropout: 0.2
  sol_head_activation: "relu"

  # Reference temperature
  T_ref_K: 298.0

# ============================================================================
# Loss: Multi-task loss weights
# ============================================================================
loss_weights:
  lambda_dft: 0.5   # Used in Stage 1
  lambda_exp: 3.0   # Fixed for Stage 2
  lambda_sol: 1.0   # Fixed for Stage 2

# ============================================================================
# Solubility: Classification settings (fixed for Stage 2)
# ============================================================================
solubility:
  class_weight_pos: 5.0
  class_weight_neg: 1.0
  decision_threshold: 0.5
  optimize_threshold: true
  threshold_metric: "f1"
  stratify: true

# ============================================================================
# Data Splits
# ============================================================================
splits:
  train_frac: 0.8
  val_frac: 0.1
  test_frac: 0.1
  split_seed: 42

# ============================================================================
# Cross-Validation: Experimental χ
# ============================================================================
cv:
  exp_chi_k_folds: 5           # 5-fold CV for Stage 2 evaluation
  exp_chi_shuffle: true
  exp_chi_shuffle_seed: 42

# ============================================================================
# Uncertainty: MC Dropout (disabled for search)
# ============================================================================
uncertainty:
  mc_dropout_samples: 50       # Reduced from 150 for speed
  enable_mc_dropout: false     # Disabled during search

# ============================================================================
# Hyperparameter Optimization: Optuna settings
# ============================================================================
hparam_search:
  # Optuna settings
  n_trials: 50                 # Number of trials to run
  timeout_hours: 48            # Max time for entire search (48 hours)

  # Cross-validation for Stage 2
  n_cv_folds: 5                # 5-fold CV for robust evaluation
  stage2_epochs: 60            # Quick Stage 2 training (20 epochs)

  # Pruning (early stopping for bad trials)
  use_pruning: false           # Disabled for simplicity

  # Search space (for documentation - actual ranges defined in hparam_search_cv_aware.py)
  search_space:
    # ========== Stage 1: Pretrain on DFT ==========
    # Encoder architecture
    encoder_latent_dim: [64, 128, 256]
    encoder_hidden_dims: ["512_256", "1024_512_256", "256_128"]

    # Dropout (Stage 1)
    encoder_dropout: [0.15, 0.35]    # Continuous range
    chi_head_dropout: [0.05, 0.25]   # Continuous range

    # Learning rate (log scale)
    lr_pretrain: [1.0e-5, 3.0e-3]    # Expanded upper range

    # Regularization (log scale)
    weight_decay: [1.0e-5, 1.0e-3]

    # Batch size (Stage 1)
    batch_size_dft: [64, 128, 256]   # NEW: DFT batch size

    # ========== Stage 2: Fine-tune on Exp Chi (Strategy 1 & 2) ==========
    # Strategy 1: Higher head learning rate
    lr_finetune: [1.0e-5, 1.0e-3]    # NEW: Chi head LR (log scale)
    lr_encoder: [1.0e-6, 1.0e-3]     # NEW: Encoder LR (log scale, lower)
    freeze_encoder_stage2: [true, false]  # NEW: Whether to freeze encoder
    stage2_chi_dropout: [0.05, 0.2]  # NEW: Reduced dropout for chi head
    early_stopping_patience: [60, 80, 100, 150]  # NEW: Longer patience

    # Strategy 2: Address batch imbalance
    batch_size_exp: [1, 2, 4, 8]     # NEW: Smaller batches = more steps/epoch

  # Objective weights (for computing transfer objective)
  objective_weights:
    exp_mae: 1.5              # Weight for exp chi MAE
    sol_f1: 2.0               # Weight for solubility F1
    sol_recall: 1.5           # Weight for solubility recall
    stage1_mae: 0.1           # Small penalty for bad Stage 1

# ============================================================================
# Logging and Monitoring (minimal for search)
# ============================================================================
logging:
  console_level: "INFO"
  file_level: "DEBUG"
  log_interval: 50             # Less frequent logging
  val_interval: 5              # Validate less often
  use_tensorboard: false       # Disabled for search
  tensorboard_dir: "runs"

# ============================================================================
# Plotting: Disabled during search
# ============================================================================
plotting:
  save_png: false              # Disabled for speed
  save_pdf: false
  dpi: 300
  style: "seaborn-v0_8-paper"
  figure_size: [4.5, 4.5]
  font_size: 12
  colormap: "viridis"
  palette: "Set2"
