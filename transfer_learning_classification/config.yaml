# Configuration file for Transfer Learning: Chi Regression -> Binary Solubility Classification
#
# NOTE: The hyperparameters in this file serve as defaults used when:
#   1. Running training directly without Optuna optimization
#   2. No Optuna best parameters file is found
# During Optuna hyperparameter optimization, these values define the search space
# and the best parameters found will override these defaults.

# =============================================================================
# Data paths
# =============================================================================
data:
  dft_chi_csv: "Data/OMG_DFT_COSMOC_chi.csv"
  binary_solubility_csv: "Data/Binary_solubility.csv"
  dft_features_csv: "Data/DFT_features.csv"
  binary_features_csv: "Data/binary_features.csv"

# =============================================================================
# Feature engineering
# =============================================================================
features:
  morgan_fingerprint:
    radius: 3
    n_bits: 1024
  descriptors:
    use_all_rdkit: true  # Use all valid RDKit descriptors
  feature_mode: "fp_desc"  # Options: "fp", "desc", "fp_desc"

# =============================================================================
# Model architecture (default values)
# =============================================================================
model:
  hidden_dims: [256, 128, 64]  # Hidden layer dimensions (each layer can be different)
  dropout_rate: 0.2  # Dropout probability
  activation: "relu"  # Activation function

# =============================================================================
# Training settings (default values)
# =============================================================================
training:
  # Regression pretraining
  pretrain:
    epochs: 60
    batch_size: 128
    learning_rate: 0.001
    weight_decay: 0.0001
    optimizer: "adamw"
    loss: "mse"

  # Classification fine-tuning
  finetune:
    epochs: 500
    batch_size: 2
    learning_rate: 0.0005
    weight_decay: 0.0001
    optimizer: "adamw"
    loss: "bce"
    n_freeze_layers: 0  # Number of encoder layers to freeze

  # Data splitting
  split:
    dft_train_ratio: 0.8
    dft_val_ratio: 0.1
    dft_test_ratio: 0.1
    cv_folds: 5
    seed: 42

# =============================================================================
# MC Dropout settings
# =============================================================================
mc_dropout:
  n_samples: 100  # Number of forward passes for uncertainty estimation
  enabled: true

# =============================================================================
# Optuna hyperparameter optimization
# =============================================================================
optuna:
  study_name: "transfer_learning_solubility"
  n_trials: 1000
  direction: "maximize"  # Maximize validation F1 score
  storage: null  # Use in-memory storage (or specify SQLite path)

  # Hyperparameter search space
  search_space:
    # Feature mode
    feature_mode:
      type: "categorical"
      choices: ["fp", "desc", "fp_desc"]

    # Model architecture
    n_layers:
      type: "int"
      low: 2
      high: 7
    # Note: hidden_dim per layer is sampled independently in code
    # Each layer can be 64, 128, 256, or 512

    dropout_rate:
      type: "float"
      low: 0.1
      high: 0.5

    # Pretraining hyperparameters
    lr_pre:
      type: "loguniform"
      low: 1.0e-4
      high: 1.0e-2

    epochs_pre:
      type: "int"
      low: 60
      high: 60

    batch_pre:
      type: "categorical"
      choices: [128, 256]

    weight_decay_pre:
      type: "loguniform"
      low: 1.0e-6
      high: 1.0e-3

    # Fine-tuning hyperparameters
    lr_ft:
      type: "loguniform"
      low: 1.0e-5
      high: 1.0e-3

    epochs_ft:
      type: "int"
      low: 500
      high: 500

    batch_ft:
      type: "categorical"
      choices: [4, 8, 16, 32, 64]

    weight_decay_ft:
      type: "loguniform"
      low: 1.0e-5
      high: 1.0e-3

    # Random seed for fine-tuning data splitting (pretraining always uses fixed seed 42)
    split_seed:
      type: "int"
      low: 1
      high: 999

# =============================================================================
# Output directories
# =============================================================================
outputs:
  base_dir: "outputs"
  regression_dir: "outputs/regression"
  classification_dir: "outputs/classification"
  hyperopt_dir: "hyperparameter_optimization"
  plots_dir: "outputs/plots"

# =============================================================================
# Plotting settings
# =============================================================================
plotting:
  figure_size: [4.5, 4.5]  # Default figure size in inches
  parity_size: [5.5, 4.5]  # Parity plot size for pretraining
  font_size: 12
  dpi: 600
  format: "png"
  calibration_bins_dft: 10

# =============================================================================
# Reproducibility
# =============================================================================
random_seed: 42
